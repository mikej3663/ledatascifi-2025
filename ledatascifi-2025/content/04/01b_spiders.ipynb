{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skill 3: Building a spider\n",
    "\n",
    "So, now, we can use python to open a page, pass an API query, and parse a page for the elements we're interested in.\n",
    "\n",
    "## One API hit is cool, but  do you know what's really cool?\n",
    "\n",
    "One million API hits. \n",
    "\n",
    "![](https://media.giphy.com/media/sEULHciNa7tUQ/giphy.gif)\n",
    "\n",
    "Ok, maybe not a million.[^mill] But now that you can write a `request` and modify search parameters, you might need to run a bunch of searches. \n",
    "\n",
    "[^mill]: _I've done well over a million API hits in the name of science._\n",
    "\n",
    "Scraping jobs typically fall into one of two camps:\n",
    "1. loop over URLs or some search parameters (like firm names)\n",
    "2. navigate from an initial page through subsequent pages (e.g through search results)\n",
    "\n",
    "Of course, both can be true: sometimes a spider might have a list of URLs (search for firms that filed an 8-K in 2000, then those that filed in 2001) and for each URL (year) click through all 8-Ks. \n",
    "\n",
    "```{admonition} The trick they don't want you to know\n",
    ":class: tip\n",
    "**When your job falls into the first camp - you want to loop over a list of URLs - a good way to do that is**: Define a function to do one search, then call that for each search in a list of searches.\n",
    "```\n",
    "\n",
    "## A silly spider\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_itunes(search_term):\n",
    "    '''Run one simple iTunes search'''\n",
    "    \n",
    "    base_url = 'https://itunes.apple.com/search'\n",
    "    search_parameters = {'term': search_term}\n",
    "    \n",
    "    r = requests.get(base_url, params = search_parameters)\n",
    "    \n",
    "    results_df = pd.DataFrame(r.json()['results'])\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run this one artist at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_itunes('billie eilish')      # one search at a time\n",
    "search_itunes('father john misty')  # \"another one\" - dj khaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can loop over them and <do something> (TBD, but saving the `results_df` to files is a good idea):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists = ['billie eilish','father john misty'] # you can loop over them!\n",
    "\n",
    "# download the results and save locally\n",
    "for artist in artists:\n",
    "    df = search_itunes(artist)\n",
    "    # you could do anything with the results here\n",
    "    # a good idea in many projects: save the webpage/search results\n",
    "    # even better: add the saving function inside the \"search_itunes\" fcn\n",
    "    # but this is just a toy illustration, so nothing happens\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LATER, you will want to analyze those files. Just loop over the files again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for artist in artists:\n",
    "    # load the saved file\n",
    "    # call a function you wrote to parse/analyze/reformat one file\n",
    "    # do something with the output from the parser\n",
    "    # but this is just a toy illustration, so nothing happens    \n",
    "    pass   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The main web scraping problems (and workarounds)\n",
    "\n",
    "[_Also, check out the table from a few pages ago on useful packages and tips._](01_Intro_to_scraping)\n",
    "\n",
    "```{dropdown} Issue 1:  The jobs are slow\n",
    "\n",
    "In many web scraping projects, a lot of data needs to get scraped, over thousands (or millions) (or billions) of pages. It's unlikely that you can do this all in one session. (What if your WiFi disconnects, or Windows decides to do an update, or the webpage freezes you out for a period of time?) \n",
    "\n",
    "**Solutions:**\n",
    "1. **Write code that only hits the server one time**, and saves the results to your computer. \"Step 1\" of the `search_itunes` example above does that. Then \"step 2\" uses/parses those files without going to the webpages again. \n",
    "2. **You want your spider to resume, not restart.** Ensure that your code can resume where it left off without having to restart from scratch. My usual solution: \n",
    "    ```PYTHON\n",
    "    # as I'm looping over webpages:\n",
    "    if not os.path.exists(<filename this page would get>): \n",
    "        okay_do_the_download() # whatever the function is\n",
    "    # if not, skip to the next webpage\n",
    "    ```\n",
    "3. Your spider WILL fail - you don't want it to stop. I typically use a `try-except-else` block. The `try` part accesses the url/send the API request, the `except` part prints or logs a failure to a log file, and the `else` part only executes the code I need to run after the url request if the `try` code was successful. For example, I could improve the `search_itunes` function:\n",
    "    ```PYTHON\n",
    "    if not os.path.exists(<filename this page would get>): \n",
    "        try:\n",
    "            r = requests.get(base_url, params = search_parameters)\n",
    "        except:\n",
    "            print(\"hey this didn't work! prob print better info than\")\n",
    "            print(\"this string\")\n",
    "            # or... create strings and append them to an \"error_list\",\n",
    "            # which you save to a text file or csv after the code finishes\n",
    "            # and you can look at it then\n",
    "        else:\n",
    "            results_df = pd.DataFrame(r.json()['results'])\n",
    "        ```        \n",
    "4. Your spider WILL fail - you will want to know what. You should log failures, warnings, and errors. The prior example can be adjusted to do this well. \n",
    "```\n",
    "\n",
    "```{dropdown} Issue 2: Too much speed\n",
    "\n",
    "Servers aren't free and can get overloaded. You've seen or heard of websites crashing due to high traffic - [Fandango for Star Wars - Rogue One, Black Fridays, and the Canadian Immigration site in Nov 2016](https://www.blazemeter.com/blog/biggest-web-failures-2016-and-2017-resolutions/). \n",
    "\n",
    "As such, webmasters often throttle or block computers that are sending too much traffic.\n",
    "\n",
    "**Solutions:**\n",
    "1. Slow your code down with `sleep(#)`. This is the main solution.\n",
    "2. Get API access with special permissions.\n",
    "3. If you can't slow down your spider (the code crawling the site), use multiple computers/IP addresses\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{dropdown} Issue 3: So... I'm downloading a loooot of files\n",
    "\n",
    "You are! \n",
    "\n",
    "It's important to save them in an organized way. There is no \"one way\", and the directory/storage scheme I choose depends on the job. The main thing is that you probably want two abilities after the download:\n",
    "1. If you sequentially open all files, can you tell what they are? (E.g. the firm, the year, the form type.)\n",
    "2. If you want to only open some files, can you do that without opening all files? (E.g. only open 10-Ks but not 10-Qs.)\n",
    "\n",
    "How you achieve these is somewhat up to you but you basically have two choices (and these can work in tandom):\n",
    "\n",
    "**Solution 1: Build the folder structure so that the path to the file tells you what you need to know.**\n",
    "\n",
    " E.g. `/gvkey_10145/10-ks/2008/934573495-923875934.txt` is \"obviously\" the 2008 10-K for firm 10145, and you know this without needing to open the file and even though the filename itself is not very clear. \n",
    "\n",
    "**Solution 2: Keep a master list of documents**\n",
    "\n",
    "Sometimes it's not possible or reasonable to know exactly how to build the directory in advance. For example, forms filed to the SEC in 2008 are often for fiscal year 2007. So what does the \"2008\" folder mean? How can you tell before running everything? So maybe you just download all the 10-ks for that firm inside the `/gvkey_10145/10-ks/` folder.\n",
    "\n",
    "To find the 2008 10-K, you'd open up a master list of documents that contains variables with enough info to assemble the path to each file, and info about each file. Then you can `query(\"form='10-K' & fyear=2008\")`, assemble the filename, and run your code. \n",
    "\n",
    "This master list must either be assembled before you run your spider (like in Assignment 5), as you run the spider (collect the info and save it as you go), or after the download you run some code one time to assemble it (either using their paths a la `/gvkey_10145/10-ks/2008/934573495-923875934.txt`, or open every single file to extract the info about the document).\n",
    "````\n",
    "\n",
    "### Summary\n",
    "\n",
    "You can combine all this discussion into a \"general structure\" for spiders. For each page you want to visit, you need to know \n",
    "1. The URL (or the search term)\n",
    "2. The folder and filename you want to save it to\n",
    "\n",
    "And then, for each page you want to visit you'll run this:\n",
    "```python\n",
    "def one_search(<the url>,<filename this page would get>):\n",
    "    if not os.path.exists(<filename this page would get>): \n",
    "        try:\n",
    "            r = requests.get(<the url>)\n",
    "        except:\n",
    "            # log the error somehow\n",
    "        else:\n",
    "            # save the results, I typically save the RAW source \n",
    "            sleep(3) # be nice to server\n",
    "```\n",
    "\n",
    "And that gets run within some loop.\n",
    "\n",
    "```python\n",
    "for url in urls:\n",
    "    filename_to_save = <some function of the url>\n",
    "    one_search(url,filename_to_save)\n",
    "```        \n",
    "\n",
    "This structure is pretty adaptable depending on the nature of the problem and the input data you have that yields the list of URLs to visit. \n",
    "\n",
    "\n",
    "### Would you like another tutorial to try?\n",
    "\n",
    "Again, Greg Reda has a nice [walkthrough](https://nbviewer.jupyter.org/github/nealcaren/ScrapingData/blob/master/Notebooks/Bonus_Downloading.ipynb) discussing building a robust program to download a list and incorporating many of the elements we've talked about here. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Credits\n",
    "\n",
    "- STAT545, as always\n",
    "- [Neal Caren](https://nealcaren.org/)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
